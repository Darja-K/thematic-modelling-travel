{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uuuniUXTjGuH"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGjQSVRDpKn4",
    "outputId": "9e6a1534-14a9-4c9c-8ef1-4ff624bedc36"
   },
   "outputs": [],
   "source": [
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "df = pd.read_csv('Go Russia.csv')\n",
    "\n",
    "# 2. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:\")\n",
    "print(df.head())\n",
    "print(\"\\n–ö–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞:\")\n",
    "print(df.columns)\n",
    "print(\"\\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ:\")\n",
    "df.info()\n",
    "\n",
    "# 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "df = df.dropna(subset=['–¢–µ–∫—Å—Ç'])\n",
    "df['–¢–µ–∫—Å—Ç'] = df['–¢–µ–∫—Å—Ç'].astype(str)\n",
    "\n",
    "# 4. –†–∞—Å—Å—á—ë—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "df['char_length'] = df['–¢–µ–∫—Å—Ç'].apply(len)  # –î–ª–∏–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö\n",
    "df['word_count'] = df['–¢–µ–∫—Å—Ç'].apply(lambda x: len(x.split()))  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤\n",
    "df['unique_words'] = df['–¢–µ–∫—Å—Ç'].apply(lambda x: len(set(x.split())))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "df['lexical_diversity'] = df.apply(\n",
    "    lambda x: x['unique_words'] / x['word_count'] if x['word_count'] > 0 else 0,\n",
    "    axis=1\n",
    ")  # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ\n",
    "df['avg_word_length'] = df['–¢–µ–∫—Å—Ç'].apply(\n",
    "    lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0\n",
    ")  # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞\n",
    "\n",
    "# 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É\n",
    "total_texts = len(df)\n",
    "avg_char_length = df['char_length'].mean()\n",
    "avg_word_count = df['word_count'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤: {total_texts}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö: {avg_char_length:.2f}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ: {avg_word_count:.2f}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {df['unique_words'].mean():.2f}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω–µ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {df['lexical_diversity'].mean():.4f}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞: {df['avg_word_length'].mean():.2f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTNyDZFQpOjP",
    "outputId": "23ec6337-ca25-4743-9382-ecfb9e1d058b"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download ru_core_news_sm\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"ru_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "russian_stopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609,
     "referenced_widgets": [
      "d3fa49b30c254db69d8e748aa0404ad6",
      "e5e3a9b6c862432fa523caf1fe62d5ef",
      "6480f9c00d794580bb0295baadcc62bb",
      "73205c2b18d043678c19f4c7f3fde4d5",
      "4fd941f880a549269715eeed7d1207bf",
      "1d3535262b5b4423a59e0b9b6aebea7a",
      "ff90dde1c9b148b9a9369d82ba83105d",
      "2f163a699e8c4a9c9fbe9ba187a60d63",
      "1db915ec85b447db90cdb671dc147c5d",
      "991a0ab1e82e4540b0e5a45dee6f8d75",
      "e28e9deeea024ef582909d19c38ca725",
      "3ed811300d404f5cb60b8052b0a3e773",
      "35820a85bb5746dfbe222682a1a3bf2d",
      "9f5f2cb9deef4c46b11d037b3a301fe3",
      "8911810eb0eb4e14834f77e428edc7b3",
      "88c45150971b477ab3c20af664a99ac5",
      "9e6f3e454d2d459ba051c82a087b8e42",
      "9ba2acd239144848a1949a01ef2e8686",
      "311efa943e1743419ab14a19fecc10e1",
      "62f1e78986504686a20fdd04905ffa28",
      "35d06c555dc5430bbb2894822e675cd3",
      "a02b5b9116174c29a14e7f99bf60f46c",
      "ea636139a8c14284bb49c2eb297c4501",
      "49b941ebe1884cc880afb7d7baae2ee5",
      "ef80f4e36acd43d7ad76f82cc42480a3",
      "0d65b02d28074b14a2b2e87d141dd758",
      "46bba02d30f441c88bcd3bdcb443846a",
      "15474b3b307d4303a850ea431a9b9576",
      "0891ae031cfe44eeb2d7772f035feea2",
      "ef94b529f6854e40b99d5d5356e41331",
      "0685c6f77c394bfd8862be217eb66a7b",
      "835e38d708c64b52adb573aef3d54ec7",
      "c583a0fb592b42ddb32e35a45c12bc42",
      "5d626a2fc6f84eeb9f68fc943342df27",
      "af5c3a535ced401992b81a7b8cca387a",
      "42f904037c9b471f8777c500576879b4",
      "273dcf4ef10b4204b87e8600a57e8bbe",
      "534ade4182844cf3bc22ff3369e7181e",
      "9f11cbf2c0764366afb55b87f327d8e8",
      "153d2faf2be6494b81fd80019551158c",
      "413fa55afdcb4c749cfc1935cc98105a",
      "08d822e9d2254b7fb2589ee0405fcf87",
      "0b7258f013024afea31deb07f0d38ef1",
      "473bb6db28b54b6f9c3b980a8edba272",
      "8761d2bf23b043fb92a0112c432e56fb",
      "4bc88266a0704c48b605a932adbd9b0c",
      "67d4e662e08044368a83dd6cde95ab71",
      "9e2f2ff6904646438ca5568e07a2fd39",
      "9b1f3edfba9a4a52bbf1d74d11ffde15",
      "784910f6393843fda029015697aad3e5",
      "39a5bf454bb943ac8121b02432b988ea",
      "6c4b99b0a1b44b77a7e0bedfbee35cd9",
      "781b92e80eee4bd69e09db42590d3553",
      "b5a9747767dc4cb3b804063962d74d09",
      "d56ff7d57e32458d92846adf3a6059b8",
      "91108965eef54d5db089a24238eb5236",
      "3815003be9c447f883d1eee87e2e0e3f",
      "8afa681bbf96479c9d65cfa836ab8ec5",
      "ac3eb7764ad148459726d78f49e4cb9d",
      "2c5211d88aea43998dda6204a9c00858",
      "6d2e6192a7784daea8f542f5b33582d6",
      "db5df870a716494a9512130c599818a1",
      "378f11c7e9df4f27a5c41c5187c3a9d2",
      "ef8646083f814d489e850861d56ac907",
      "7a3eb2973b7b41779340105e00ad5789",
      "5bd5649081cc4d3dbecdcee6eada2390",
      "e50953e6c3e943d99792f3e6fe83d5d3",
      "1d7569304f4c41078d852ddd9444c054",
      "7889c40f5a8b4784be977257b429e626",
      "e74caa7c6ef842b2940345fb3074a64a",
      "bfb3adbf63d8481ea08971788ec91c98",
      "ca071dc4a45547248804ff9004409a24",
      "b735c2765dc848a8b5cb028bfd6b51d5",
      "d07974a12d5a4e7fbaf78ecdcae21e48",
      "6cf212263e4b4c11a5cb892e530f064d",
      "b3f7219a43e04c59bf346e1162c88bc5",
      "0ab9559c0af849fdbecfc2ad308b17ab",
      "02d141932455474b8dd465eb19575b67",
      "9a6dda855ff643d8a435d640d53fc5fc",
      "306e8a6ed8094fd88addf36fdb1f953b",
      "a2b46ac7c8dc41319e4cb6ec3b542cc3",
      "0a6ddd8d40ea4f37b83bfcc153d4463d",
      "098d34abb30e4b7cac243a848c776887",
      "4c18b23b1e74413290fd1fc971d46c25",
      "fd2e7a274917405b9668e156f034d616",
      "453e780d336241a2a4dfddec0bedcb58",
      "7beafc10b0174898a49f4c05add1db3d",
      "0019a71832ac42c7ab9d9ea1591c76a8",
      "4b30903e34794d619a8fc26aa749206e",
      "ac60efc582d14887bd97fa4a0e8f629e",
      "74ce5630255d4ae28472bc8a3c1e11ce",
      "010f648ea9434eb38769fadaad38fc9d",
      "0538b3d4d29440fdbdc57d6dde54400c",
      "5e991426a1174b25a86f5209cd16f6fd",
      "75e4b00f92064287bcf7cb49f1fd10c0",
      "6073e934b26d4ef0a812b4056ea83234",
      "c63beedad5d44608b9234603a092f937",
      "a0c153b32d19403ab1a7006bc1054379",
      "79d5b436235d45069d3d6ef4b5611e23",
      "b42f39f95d8344858bf1f99b932777f4",
      "ab124320df724f7cb2cb30cdb76baa84",
      "7b268615d493441eab0927cf89cbaf04",
      "36668c72177d4d8a8fc7a23e915db2e7",
      "53cfce527aca4c1baca1f8ec9a9853c5",
      "7881350fc7154e8c849219a1c08bd1d6",
      "0179a0661fa64a05b46f85fad1bd7339",
      "f83f8e5ade46487e95d6a853232b875a",
      "f90e27523136438e9b7c85cc8d616656",
      "2a5f34d03f594a1caf0809a23c450034",
      "388cecfe03dd430198d03492e9b96c75",
      "b7c822882fba4024b69177fb58beafc0",
      "e8af82abfed44fd2a277ffd51f25afb8",
      "38156280732346b5a3a68f37bc576b31",
      "2cd9857e25c64a41ac7884c4c1fcf1cc",
      "97beec7cca4547cc9f8918423f7edd9f",
      "9a35a3688cfa42a59503b356720b12a7",
      "7d34606b6465429d9a5d71a79b04e277",
      "50a775a157db47bd8e12fcb2db947096",
      "18126dcd4e184d2b815ae0acb73ea049",
      "229c8e8449a64e539d6d0fa50327e96e",
      "a75d51b7801f4bb49dc147535c3beb83"
     ]
    },
    "id": "3EwpXF5GpkCF",
    "outputId": "bc25e6e4-c848-4867-808d-b42dc41aed88"
   },
   "outputs": [],
   "source": [
    "#  –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "def preprocess_text(text):\n",
    "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    text = text.lower()\n",
    "\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ü–∏—Ñ—Ä\n",
    "    text = re.sub(r'[^–∞-—è—ë\\s]', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é spacy\n",
    "    doc = nlp(text)\n",
    "    lemmas = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.text not in russian_stopwords\n",
    "        and len(token.text) > 2\n",
    "        and token.is_alpha\n",
    "    ]\n",
    "\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "#  –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "print(\"\\n–ù–∞—á–∞–ª–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞...\")\n",
    "combined_texts = (df['–ó–∞–≥–æ–ª–æ–≤–æ–∫'].fillna('') + ' ' + df['–¢–µ–∫—Å—Ç'].fillna(''))\n",
    "tqdm.pandas(desc=\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "df['processed_text'] = combined_texts.progress_apply(preprocess_text)\n",
    "print(\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "\n",
    "#  –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
    "texts = df['processed_text'].tolist()\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "embeddings = model.encode(texts, show_progress_bar=True, batch_size=32)\n",
    "print(f\"–°–æ–∑–¥–∞–Ω–æ {len(embeddings)} –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIA7P-Fnl6Qv",
    "outputId": "919a1fea-ba91-4809-de6f-4280a6809d34"
   },
   "outputs": [],
   "source": [
    "# —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
    "print(combined_texts.iloc[0])\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
    "print(df['processed_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnVXsqD0Hxdj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HzQ5sIY-vYGI",
    "outputId": "b36a09d9-a943-4444-d1a5-cc65f1c08088"
   },
   "outputs": [],
   "source": [
    "# ---- –§–£–ù–ö–¶–ò–Ø –°–ò–õ–£–≠–¢–ù–û–ì–û –ì–†–ê–§–ò–ö–ê ----\n",
    "def plot_silhouettes(samples_silhouettes, labels, n_clusters, silhouette_avg):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = samples_silhouettes[labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = plt.cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                         0, ith_cluster_silhouette_values,\n",
    "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  # 10 for spacing between silhouette plots\n",
    "\n",
    "    ax.set_title(\"Silhouette plot\")\n",
    "    ax.set_xlabel(\"Silhouette score\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(np.arange(-0.2, 1.1, 0.2))\n",
    "    plt.show()\n",
    "\n",
    "# ---- –ü–û–ò–°–ö –û–ü–¢–ò–ú–ê–õ–¨–ù–û–ì–û –ß–ò–°–õ–ê –ö–õ–ê–°–¢–ï–†–û–í, –°–ò–õ–£–≠–¢–ù–´–ï –ì–†–ê–§–ò–ö–ò ----\n",
    "K = range(2, 8)\n",
    "inertias = []\n",
    "silhouette_avgs = []\n",
    "\n",
    "for k in K:\n",
    "    print(f\"\\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {k} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette_avg = silhouette_score(embeddings, labels)\n",
    "    silhouette_avgs.append(silhouette_avg)\n",
    "    inertias.append(inertia)\n",
    "    print(f\"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {k} | inertia: {inertia:.0f} | Silhouette avg: {silhouette_avg:.3f}\")\n",
    "\n",
    "    # –°—á–∏—Ç–∞–µ–º —Å–∏–ª—É—ç—Ç—ã –¥–ª—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫\n",
    "    samples_silhouettes = silhouette_samples(embeddings, labels)\n",
    "\n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏–ª—É—ç—Ç–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ k\n",
    "    plot_silhouettes(samples_silhouettes, labels, k, silhouette_avg)\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫–∏ –ª–æ–∫—Ç—è –∏ —Å–∏–ª—É—ç—Ç–∞\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K, inertias, marker='o')\n",
    "plt.xlabel('–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('–ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K, silhouette_avgs, marker='o')\n",
    "plt.xlabel('–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k)')\n",
    "plt.ylabel('–°—Ä–µ–¥–Ω–∏–π —Å–∏–ª—É—ç—Ç')\n",
    "plt.title('Silhouette Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –í–´–ë–û–† –ß–ò–°–õ–ê –ö–õ–ê–°–¢–ï–†–û–í –ø–æ –∞–Ω–∞–ª–∏–∑—É –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "NUM_CLUSTERS = int(input(\"\\n–í–≤–µ–¥–∏—Ç–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤: \"))\n",
    "\n",
    "# 4. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —á–∏—Å–ª–æ–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "print(f\"\\n–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å {NUM_CLUSTERS} –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏...\")\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ø–ª–æ—Å–∫–æ—Å—Ç–∏\n",
    "print(\"–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=40)\n",
    "emb_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "df['tsne_1'] = emb_2d[:, 0]\n",
    "df['tsne_2'] = emb_2d[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x=df['tsne_1'], y=df['tsne_2'], hue=clusters, palette='tab10', legend='full')\n",
    "plt.title('–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –æ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è—Ö')\n",
    "plt.show()\n",
    "\n",
    "# 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É\n",
    "df.to_csv('Go_Russia_clusters.csv', index=False)\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Go_Russia_clusters.csv\")\n",
    "\n",
    "# 7. –ü—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:\")\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_size = len(df[df['cluster'] == i])\n",
    "    print(f\"\\nCluster {i} ({cluster_size} —Ç–µ–∫—Å—Ç–æ–≤):\")\n",
    "    cluster_examples = df[df['cluster'] == i]['–ó–∞–≥–æ–ª–æ–≤–æ–∫'].head(5).tolist()\n",
    "    for j, example in enumerate(cluster_examples, 1):\n",
    "        print(f' {j}. {example}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azkT5UC3u-VU",
    "outputId": "a1150e08-6403-45e2-e274-66eb172740c4"
   },
   "outputs": [],
   "source": [
    "!pip install stop-words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6Us5vN4uWA2O",
    "outputId": "53d1a706-233f-457d-86e4-21cc73954759"
   },
   "outputs": [],
   "source": [
    "# —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "def show_cluster_info(df, text_col='processed_text', title_col='–ó–∞–≥–æ–ª–æ–≤–æ–∫', n_top_words=15):\n",
    "    russian_stop_words = get_stop_words('russian')\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"–ê–ù–ê–õ–ò–ó –ö–õ–ê–°–¢–ï–†–û–í\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    cluster_stats = []\n",
    "\n",
    "    for c in sorted(df['cluster'].unique()):\n",
    "        cluster_df = df[df['cluster'] == c]\n",
    "        cluster_size = len(cluster_df)\n",
    "        cluster_stats.append((c, cluster_size))\n",
    "\n",
    "        print(f\"\\n### –ö–ª–∞—Å—Ç–µ—Ä {c} ({cluster_size} —Ç–µ–∫—Å—Ç–æ–≤, {cluster_size/total_texts:.1%} –¥–∞–Ω–Ω—ã—Ö):\")\n",
    "\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "        cluster_texts = cluster_df[text_col].dropna().astype(str).tolist()\n",
    "\n",
    "        if not cluster_texts:\n",
    "            print('–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!')\n",
    "            continue\n",
    "\n",
    "        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ TF-IDF\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(max_features=500, stop_words=russian_stop_words)\n",
    "            tfidf_matrix = vectorizer.fit_transform(cluster_texts)\n",
    "            words = vectorizer.get_feature_names_out()\n",
    "            scores = tfidf_matrix.mean(axis=0).A1\n",
    "            indices = scores.argsort()[::-1][:n_top_words]\n",
    "            top_words = [words[i] for i in indices]\n",
    "\n",
    "            print(f'üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {\", \".join(top_words)}')\n",
    "        except ValueError:\n",
    "            print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞)\")\n",
    "\n",
    "        # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        titles = cluster_df[title_col].dropna().astype(str).tolist()[:20]\n",
    "        print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:\")\n",
    "        for i, title in enumerate(titles, 1):\n",
    "            print(f\" {i}. {title}\")\n",
    "\n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    clusters, counts = zip(*sorted(cluster_stats, key=lambda x: x[0]))\n",
    "    plt.bar([str(c) for c in clusters], counts, color=plt.cm.tab10.colors[:len(clusters)])\n",
    "    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º')\n",
    "    plt.xlabel('–ù–æ–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞')\n",
    "    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "show_cluster_info(df)\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: —Å—Ä–µ–¥–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"–°–†–ï–î–ù–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cluster_metrics = df.groupby('cluster').agg({\n",
    "    'char_length': 'mean',\n",
    "    'word_count': 'mean',\n",
    "    'lexical_diversity': 'mean',\n",
    "    'avg_word_length': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "cluster_metrics.columns = ['–ö–ª–∞—Å—Ç–µ—Ä', '–°–∏–º–≤–æ–ª—ã', '–°–ª–æ–≤–∞', '–õ–µ–∫—Å.—Ä–∞–∑–Ω–æ–æ–±—Ä.', '–°—Ä.–¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞']\n",
    "print(cluster_metrics.round(2))\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric in enumerate(['–°–∏–º–≤–æ–ª—ã', '–°–ª–æ–≤–∞', '–õ–µ–∫—Å.—Ä–∞–∑–Ω–æ–æ–±—Ä.', '–°—Ä.–¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞'], 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.barplot(x='–ö–ª–∞—Å—Ç–µ—Ä', y=metric, data=cluster_metrics, palette='tab10')\n",
    "    plt.title(f'–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {metric}')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4F0hkbL4cr21",
    "outputId": "a05b955a-f339-4517-a64b-8bf858c5e695"
   },
   "outputs": [],
   "source": [
    "# –í–´–ë–û–† –ß–ò–°–õ–ê –ö–õ–ê–°–¢–ï–†–û–í –ø–æ –∞–Ω–∞–ª–∏–∑—É –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "NUM_CLUSTERS = int(input(\"\\n–í–≤–µ–¥–∏—Ç–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤: \"))\n",
    "\n",
    "# 4. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —á–∏—Å–ª–æ–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "print(f\"\\n–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å {NUM_CLUSTERS} –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏...\")\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ø–ª–æ—Å–∫–æ—Å—Ç–∏\n",
    "print(\"–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=40)\n",
    "emb_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "df['tsne_1'] = emb_2d[:, 0]\n",
    "df['tsne_2'] = emb_2d[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x=df['tsne_1'], y=df['tsne_2'], hue=clusters, palette='tab10', legend='full')\n",
    "plt.title('–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –æ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è—Ö')\n",
    "plt.show()\n",
    "\n",
    "# 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É\n",
    "df.to_csv('Go_Russia_clusters.csv', index=False)\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Go_Russia_clusters.csv\")\n",
    "\n",
    "# 7. –ü—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:\")\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_size = len(df[df['cluster'] == i])\n",
    "    print(f\"\\nCluster {i} ({cluster_size} —Ç–µ–∫—Å—Ç–æ–≤):\")\n",
    "    cluster_examples = df[df['cluster'] == i]['–ó–∞–≥–æ–ª–æ–≤–æ–∫'].head(5).tolist()\n",
    "    for j, example in enumerate(cluster_examples, 1):\n",
    "        print(f' {j}. {example}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "64RPyyhrc_aU",
    "outputId": "f6b750a8-e7fa-4a1c-eb4d-3c0c4d047200"
   },
   "outputs": [],
   "source": [
    "# —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "def show_cluster_info(df, text_col='processed_text', title_col='–ó–∞–≥–æ–ª–æ–≤–æ–∫', n_top_words=15):\n",
    "    russian_stop_words = get_stop_words('russian')\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"–ê–ù–ê–õ–ò–ó –ö–õ–ê–°–¢–ï–†–û–í\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    cluster_stats = []\n",
    "\n",
    "    for c in sorted(df['cluster'].unique()):\n",
    "        cluster_df = df[df['cluster'] == c]\n",
    "        cluster_size = len(cluster_df)\n",
    "        cluster_stats.append((c, cluster_size))\n",
    "\n",
    "        print(f\"\\n### –ö–ª–∞—Å—Ç–µ—Ä {c} ({cluster_size} —Ç–µ–∫—Å—Ç–æ–≤, {cluster_size/total_texts:.1%} –¥–∞–Ω–Ω—ã—Ö):\")\n",
    "\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "        cluster_texts = cluster_df[text_col].dropna().astype(str).tolist()\n",
    "\n",
    "        if not cluster_texts:\n",
    "            print('–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!')\n",
    "            continue\n",
    "\n",
    "        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ TF-IDF\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(max_features=500, stop_words=russian_stop_words)\n",
    "            tfidf_matrix = vectorizer.fit_transform(cluster_texts)\n",
    "            words = vectorizer.get_feature_names_out()\n",
    "            scores = tfidf_matrix.mean(axis=0).A1\n",
    "            indices = scores.argsort()[::-1][:n_top_words]\n",
    "            top_words = [words[i] for i in indices]\n",
    "\n",
    "            print(f'üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {\", \".join(top_words)}')\n",
    "        except ValueError:\n",
    "            print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞)\")\n",
    "\n",
    "        # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        titles = cluster_df[title_col].dropna().astype(str).tolist()[:20]\n",
    "        print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:\")\n",
    "        for i, title in enumerate(titles, 1):\n",
    "            print(f\" {i}. {title}\")\n",
    "\n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    clusters, counts = zip(*sorted(cluster_stats, key=lambda x: x[0]))\n",
    "    plt.bar([str(c) for c in clusters], counts, color=plt.cm.tab10.colors[:len(clusters)])\n",
    "    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º')\n",
    "    plt.xlabel('–ù–æ–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞')\n",
    "    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "show_cluster_info(df)\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: —Å—Ä–µ–¥–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"–°–†–ï–î–ù–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cluster_metrics = df.groupby('cluster').agg({\n",
    "    'char_length': 'mean',\n",
    "    'word_count': 'mean',\n",
    "    'lexical_diversity': 'mean',\n",
    "    'avg_word_length': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "cluster_metrics.columns = ['–ö–ª–∞—Å—Ç–µ—Ä', '–°–∏–º–≤–æ–ª—ã', '–°–ª–æ–≤–∞', '–õ–µ–∫—Å.—Ä–∞–∑–Ω–æ–æ–±—Ä.', '–°—Ä.–¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞']\n",
    "print(cluster_metrics.round(2))\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric in enumerate(['–°–∏–º–≤–æ–ª—ã', '–°–ª–æ–≤–∞', '–õ–µ–∫—Å.—Ä–∞–∑–Ω–æ–æ–±—Ä.', '–°—Ä.–¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞'], 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.barplot(x='–ö–ª–∞—Å—Ç–µ—Ä', y=metric, data=cluster_metrics, palette='tab10')\n",
    "    plt.title(f'–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {metric}')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
